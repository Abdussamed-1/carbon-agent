# scripts/analyze_esg_data.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class ESGDataAnalyzer:
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.df = None
        
    def load_data(self):
        """CSV dosyasÄ±nÄ± yÃ¼kle"""
        print("ğŸ“Š ESG verisi yÃ¼kleniyor...")
        self.df = pd.read_csv(self.csv_path)
        print(f"âœ… {len(self.df)} satÄ±r, {len(self.df.columns)} sÃ¼tun yÃ¼klendi")
        return self.df
    
    def analyze_data_quality(self):
        """Veri kalitesini analiz et"""
        print("\nğŸ” VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°")
        print("=" * 50)
        
        # Temel bilgiler
        print(f"ğŸ“ˆ Toplam kayÄ±t sayÄ±sÄ±: {len(self.df):,}")
        print(f"ğŸ“Š SÃ¼tun sayÄ±sÄ±: {len(self.df.columns)}")
        print(f"ğŸ“… Tarih aralÄ±ÄŸÄ±: {self.df['Date'].min()} - {self.df['Date'].max()}")
        
        # Eksik deÄŸerler
        print("\nâŒ EKSIK DEÄERLER:")
        missing_data = self.df.isnull().sum()
        missing_percent = (missing_data / len(self.df)) * 100
        
        for col in self.df.columns:
            if missing_data[col] > 0:
                print(f"  {col}: {missing_data[col]:,} ({missing_percent[col]:.1f}%)")
        
        # Benzersiz deÄŸerler
        print("\nğŸ¢ BENZERSIZ DEÄERLER:")
        print(f"  Benzersiz ÅŸirket sayÄ±sÄ±: {self.df['Company'].nunique():,}")
        print(f"  Benzersiz ticker sayÄ±sÄ±: {self.df['Ticker'].nunique():,}")
        print(f"  Benzersiz Ã¼lke sayÄ±sÄ±: {self.df['Country'].nunique()}")
        print(f"  Benzersiz bÃ¶lge sayÄ±sÄ±: {self.df['Region'].nunique()}")
        print(f"  Benzersiz peer group sayÄ±sÄ±: {self.df['Peer_group_root'].nunique()}")
        
        # Skor daÄŸÄ±lÄ±mlarÄ±
        print("\nğŸ“Š SKOR Ä°STATÄ°STÄ°KLERÄ°:")
        score_columns = ['total_esg_score', 'environment_score', 'social_score', 'governance_score']
        for col in score_columns:
            if col in self.df.columns:
                stats = self.df[col].describe()
                print(f"  {col}:")
                print(f"    Min: {stats['min']:.2f}, Max: {stats['max']:.2f}")
                print(f"    Ortalama: {stats['mean']:.2f}, Medyan: {stats['50%']:.2f}")
        
        return missing_data, missing_percent
    
    def detect_anomalies(self):
        """Anormal deÄŸerleri tespit et"""
        print("\nâš ï¸ ANOMALI TESPÄ°TÄ°")
        print("=" * 50)
        
        anomalies = {}
        score_columns = ['total_esg_score', 'environment_score', 'social_score', 'governance_score']
        
        for col in score_columns:
            if col in self.df.columns:
                # Negatif deÄŸerler
                negative = self.df[self.df[col] < 0]
                if len(negative) > 0:
                    anomalies[f'{col}_negative'] = len(negative)
                    print(f"  {col}: {len(negative)} negatif deÄŸer")
                
                # 100'den bÃ¼yÃ¼k deÄŸerler (ESG skorlarÄ± genelde 0-100 arasÄ±)
                over_100 = self.df[self.df[col] > 100]
                if len(over_100) > 0:
                    anomalies[f'{col}_over_100'] = len(over_100)
                    print(f"  {col}: {len(over_100)} deÄŸer 100'den bÃ¼yÃ¼k")
                
                # AÅŸÄ±rÄ± deÄŸerler (IQR yÃ¶ntemi)
                Q1 = self.df[col].quantile(0.25)
                Q3 = self.df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
                if len(outliers) > 0:
                    anomalies[f'{col}_outliers'] = len(outliers)
                    print(f"  {col}: {len(outliers)} aÅŸÄ±rÄ± deÄŸer")
        
        # MÃ¼kerrer kayÄ±tlar
        duplicates = self.df.duplicated(['Company', 'Date']).sum()
        if duplicates > 0:
            anomalies['duplicates'] = duplicates
            print(f"  MÃ¼kerrer kayÄ±t: {duplicates}")
        
        return anomalies
    
    def clean_data(self):
        """Veriyi temizle"""
        print("\nğŸ§¹ VERÄ° TEMÄ°ZLEME")
        print("=" * 50)
        
        original_len = len(self.df)
        cleaned_df = self.df.copy()
        
        # 1. Eksik temel bilgilere sahip kayÄ±tlarÄ± kaldÄ±r
        essential_columns = ['Company', 'Ticker', 'total_esg_score']
        before_essential = len(cleaned_df)
        cleaned_df = cleaned_df.dropna(subset=essential_columns)
        removed_essential = before_essential - len(cleaned_df)
        if removed_essential > 0:
            print(f"  âŒ Temel bilgi eksik {removed_essential} kayÄ±t kaldÄ±rÄ±ldÄ±")
        
        # 2. Negatif ESG skorlarÄ±nÄ± kaldÄ±r
        score_columns = ['total_esg_score', 'environment_score', 'social_score', 'governance_score']
        for col in score_columns:
            if col in cleaned_df.columns:
                before_negative = len(cleaned_df)
                cleaned_df = cleaned_df[cleaned_df[col] >= 0]
                removed_negative = before_negative - len(cleaned_df)
                if removed_negative > 0:
                    print(f"  âŒ {col} negatif {removed_negative} kayÄ±t kaldÄ±rÄ±ldÄ±")
        
        # 3. 100'den bÃ¼yÃ¼k skorlarÄ± 100 ile sÄ±nÄ±rla
        for col in score_columns:
            if col in cleaned_df.columns:
                over_100_count = (cleaned_df[col] > 100).sum()
                if over_100_count > 0:
                    cleaned_df[col] = cleaned_df[col].clip(upper=100)
                    print(f"  ğŸ”§ {col} {over_100_count} deÄŸer 100 ile sÄ±nÄ±rlandÄ±")
        
        # 4. MÃ¼kerrer kayÄ±tlarÄ± kaldÄ±r
        before_duplicates = len(cleaned_df)
        cleaned_df = cleaned_df.drop_duplicates(['Company', 'Date'])
        removed_duplicates = before_duplicates - len(cleaned_df)
        if removed_duplicates > 0:
            print(f"  âŒ {removed_duplicates} mÃ¼kerrer kayÄ±t kaldÄ±rÄ±ldÄ±")
        
        # 5. Eksik deÄŸerleri doldur
        # Categorical columns
        categorical_fills = {
            'Country': 'Unknown',
            'Region': 'Unknown', 
            'Peer_group_root': 'Other'
        }
        
        for col, fill_value in categorical_fills.items():
            if col in cleaned_df.columns:
                filled_count = cleaned_df[col].isnull().sum()
                if filled_count > 0:
                    cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                    print(f"  ğŸ”§ {col} {filled_count} eksik deÄŸer '{fill_value}' ile dolduruldu")
        
        # Numerical columns - median ile doldur
        numerical_columns = ['environment_score', 'social_score', 'governance_score',
                           'scl_environment_score', 'scl_social_score', 'scl_governance_score',
                           'wght_environment_score', 'wght_social_score', 'wght_governance_score']
        
        for col in numerical_columns:
            if col in cleaned_df.columns:
                filled_count = cleaned_df[col].isnull().sum()
                if filled_count > 0:
                    median_value = cleaned_df[col].median()
                    cleaned_df[col] = cleaned_df[col].fillna(median_value)
                    print(f"  ğŸ”§ {col} {filled_count} eksik deÄŸer median ({median_value:.2f}) ile dolduruldu")
        
        # SonuÃ§larÄ± gÃ¶ster
        final_len = len(cleaned_df)
        removed_total = original_len - final_len
        print(f"\nâœ… Temizleme tamamlandÄ±:")
        print(f"  BaÅŸlangÄ±Ã§: {original_len:,} kayÄ±t")
        print(f"  BitiÅŸ: {final_len:,} kayÄ±t")
        print(f"  KaldÄ±rÄ±lan: {removed_total:,} kayÄ±t ({(removed_total/original_len)*100:.1f}%)")
        
        return cleaned_df
    
    def create_training_dataset(self, cleaned_df, output_path):
        """Fine-tuning iÃ§in optimize edilmiÅŸ dataset oluÅŸtur"""
        print("\nğŸ“ FINE-TUNING DATASET OLUÅTURMA")
        print("=" * 50)
        
        # SkorlarÄ± kategorize et (daha iyi Ã¶rnekler iÃ§in)
        def categorize_score(score):
            if score >= 80: return "Excellent"
            elif score >= 60: return "Good"
            elif score >= 40: return "Average"
            else: return "Poor"
        
        # Her ÅŸirket iÃ§in en son kaydÄ± al (Ã§oklu zaman serisi varsa)
        latest_data = cleaned_df.sort_values('Date').groupby('Company').tail(1).reset_index(drop=True)
        
        # Kategori sÃ¼tunlarÄ± ekle
        latest_data['esg_category'] = latest_data['total_esg_score'].apply(categorize_score)
        latest_data['env_category'] = latest_data['environment_score'].apply(categorize_score)
        latest_data['social_category'] = latest_data['social_score'].apply(categorize_score)
        latest_data['gov_category'] = latest_data['governance_score'].apply(categorize_score)
        
        # Ã‡eÅŸitlilik iÃ§in stratified sampling
        # Her kategoriden Ã¶rnek al
        training_samples = []
        
        for category in ['Excellent', 'Good', 'Average', 'Poor']:
            category_data = latest_data[latest_data['esg_category'] == category]
            if len(category_data) > 0:
                # Her kategoriden maksimum 500 Ã¶rnek al
                sample_size = min(500, len(category_data))
                sampled = category_data.sample(n=sample_size, random_state=42)
                training_samples.append(sampled)
        
        final_training_data = pd.concat(training_samples, ignore_index=True)
        
        # Kaydet
        final_training_data.to_csv(output_path, index=False)
        
        print(f"âœ… Fine-tuning dataset oluÅŸturuldu:")
        print(f"  Dosya: {output_path}")
        print(f"  Toplam Ã¶rnek: {len(final_training_data):,}")
        print(f"  Kategori daÄŸÄ±lÄ±mÄ±:")
        
        category_dist = final_training_data['esg_category'].value_counts()
        for cat, count in category_dist.items():
            print(f"    {cat}: {count:,}")
        
        return final_training_data
    
    def generate_sample_questions(self, df, n_samples=50):
        """Fine-tuning iÃ§in Ã¶rnek sorular oluÅŸtur"""
        questions = []
        
        # FarklÄ± soru tÃ¼rleri
        question_templates = [
            "{company} ÅŸirketinin ESG performansÄ±nÄ± analiz eder misin?",
            "{company} iÃ§in ESG iyileÅŸtirme stratejisi Ã¶nerir misin?",
            "{company} ÅŸirketinin Ã§evresel performansÄ± nasÄ±l?",
            "{company} ÅŸirketinin sosyal sorumluluk skoru hakkÄ±nda bilgi verir misin?",
            "{company} ÅŸirketinin kurumsal yÃ¶netim kalitesi nasÄ±l?",
            "{region} bÃ¶lgesindeki ESG performansÄ± nasÄ±l?",
            "{country} Ã¼lkesindeki ÅŸirketlerin ESG durumu hakkÄ±nda bilgi verir misin?",
            "{peer_group} sektÃ¶rÃ¼nde ESG liderleri kimler?",
        ]
        
        sample_data = df.sample(n=min(n_samples, len(df)), random_state=42)
        
        for _, row in sample_data.iterrows():
            template = np.random.choice(question_templates)
            
            if "{company}" in template:
                question = template.format(company=row['Company'])
            elif "{region}" in template:
                question = template.format(region=row['Region'])
            elif "{country}" in template:
                question = template.format(country=row['Country'])
            elif "{peer_group}" in template:
                question = template.format(peer_group=row['Peer_group_root'])
            
            questions.append({
                'question': question,
                'company': row['Company'],
                'ticker': row['Ticker'],
                'total_esg_score': row['total_esg_score'],
                'context': 'fine_tuning_sample'
            })
        
        return questions


def main():
    """Ana fonksiyon"""
    csv_path = "./data/carbon-data/esg_scores.csv"
    
    try:
        # Analyzer oluÅŸtur
        analyzer = ESGDataAnalyzer(csv_path)
        
        # Veriyi yÃ¼kle
        df = analyzer.load_data()
        
        # Analiz et
        analyzer.analyze_data_quality()
        analyzer.detect_anomalies()
        
        # Temizle
        cleaned_df = analyzer.clean_data()
        
        # Fine-tuning dataset oluÅŸtur
        training_data = analyzer.create_training_dataset(
            cleaned_df, 
            "./data/esg_training_data.csv"
        )
        
        # Ã–rnek sorular oluÅŸtur
        sample_questions = analyzer.generate_sample_questions(training_data)
        
        # SonuÃ§larÄ± kaydet
        import json
        with open("./data/sample_questions.json", "w", encoding="utf-8") as f:
            json.dump(sample_questions, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ Analiz tamamlandÄ±!")
        print(f"ğŸ“ TemizlenmiÅŸ veri: ./data/esg_training_data.csv")
        print(f"ğŸ“ Ã–rnek sorular: ./data/sample_questions.json")
        
    except Exception as e:
        print(f"âŒ Hata: {e}")


if __name__ == "__main__":
    main()